{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e370be8c-75df-48d8-8216-d7acd7f78990",
   "metadata": {},
   "source": [
    "## importing needed librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dee840e-9169-458a-b352-8301bdffaeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb00c85a-9e98-4c0b-bb7b-031877e5f668",
   "metadata": {},
   "source": [
    "## code that reads a folder -> subfolder -> file and saves two text lines in a row, creates two coulmns 1. Aashar : twolines from each file, 2. shayer : name of the sub foolder and finally convert the wholr read data into dataframe and displays a sample output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ce208f13-7d61-440a-aad4-4f05ff8ca64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed files from all subfolders.\n",
      "Poets found: ['ahmad-faraz' 'akbar-allahabadi' 'allama-iqbal' 'altaf-hussain-hali'\n",
      " 'ameer-khusrau' 'bahadur-shah-zafar' 'dagh-dehlvi' 'fahmida-riaz'\n",
      " 'faiz-ahmad-faiz' 'firaq-gorakhpuri' 'gulzar' 'habib-jalib'\n",
      " 'jaan-nisar-akhtar' 'jaun-eliya' 'javed-akhtar' 'jigar-moradabadi'\n",
      " 'kaifi-azmi' 'meer-anees' 'meer-taqi-meer' 'mirza-ghalib' 'mohsin-naqvi'\n",
      " 'naji-shakir' 'naseer-turabi' 'nazm-tabatabai' 'nida-fazli'\n",
      " 'noon-meem-rashid' 'parveen-shakir' 'sahir-ludhianvi'\n",
      " 'wali-mohammad-wali' 'waseem-barelvi']\n",
      "\n",
      "--- Sample Output ---\n",
      "                                                                                      Aashar       Shayer\n",
      "0           aañkh se duur na ho dil se utar jā.egā\\nvaqt kā kyā hai guzartā hai guzar jā.egā  ahmad-faraz\n",
      "1        itnā mānūs na ho ḳhalvat-e-ġham se apnī\\ntū kabhī ḳhud ko bhī dekhegā to Dar jā.egā  ahmad-faraz\n",
      "2              Dūbte Dūbte kashtī ko uchhālā de duuñ\\nmaiñ nahīñ koī to sāhil pe utar jā.egā  ahmad-faraz\n",
      "3          zindagī terī atā hai to ye jaane vaalā\\nterī baḳhshish tirī dahlīz pe dhar jā.egā  ahmad-faraz\n",
      "4  zabt lāzim hai magar dukh hai qayāmat kā 'farāz'\\nzālim ab ke bhī na ro.egā to mar jā.egā  ahmad-faraz\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the main directory path\n",
    "main_folder_path = './dataset' \n",
    "all_dataframes = []\n",
    "\n",
    "# 2. Use os.walk to go through all levels of subfolders\n",
    "for root, dirs, files in os.walk(main_folder_path):\n",
    "    # This gets the name of the current subfolder\n",
    "    subfolder_name = os.path.basename(root)\n",
    "    \n",
    "    for filename in files:\n",
    "        file_path = os.path.join(root, filename)\n",
    "        \n",
    "        # Check if it's a file and not a hidden system file\n",
    "        if not filename.startswith('.'):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    # Read lines and strip whitespace\n",
    "                    lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "                # 3. Pair two lines with a newline \\n\n",
    "                paired_data = []\n",
    "                for i in range(0, len(lines) - 1, 2):\n",
    "                    combined = f\"{lines[i]}\\n{lines[i+1]}\"\n",
    "                    paired_data.append(combined)\n",
    "\n",
    "                # 4. Create temporary dataframe for this file\n",
    "                if paired_data: \n",
    "                    temp_df = pd.DataFrame(paired_data, columns=['Aashar'])\n",
    "                    \n",
    "                    # SET COLUMN TO SUBFOLDER NAME\n",
    "                    # If root is './dataset/Ahmad Faraz', subfolder_name will be 'Ahmad Faraz'\n",
    "                    temp_df['Shayer'] = subfolder_name\n",
    "                    \n",
    "                    all_dataframes.append(temp_df)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Skipping file {filename} in {root}: {e}\")\n",
    "\n",
    "# 5. Combine everything\n",
    "if all_dataframes:\n",
    "    df_final = pd.concat(all_dataframes, ignore_index=True)\n",
    "    print(f\"Successfully processed files from all subfolders.\")\n",
    "    print(f\"Poets found: {df_final['Shayer'].unique()}\")\n",
    "    \n",
    "    print(\"\\n--- Sample Output ---\")\n",
    "    print(df_final.head().to_string())\n",
    "else:\n",
    "    print(\"No files found to process. Please check your folder path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aac0d3-2900-4500-89eb-98971b27f626",
   "metadata": {},
   "source": [
    "## code to convert data frame into csv file and save it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "abf02767-d9fb-4455-a30d-93ffbc059a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('collected_dataset.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0433239b-3200-4fe3-a879-b029869cd7fe",
   "metadata": {},
   "source": [
    "## code to extract 2 liners (share) from a text file and store it in a data frame \"df_final\", it also gives a sample of data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "233d2ed6-9b8c-40f1-bd04-ce0dce4e22f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully combined 50 files.\n",
      "\n",
      "--- Sample Output ---\n",
      "                                                                                      Aashar       Shayer\n",
      "0           aañkh se duur na ho dil se utar jā.egā\\nvaqt kā kyā hai guzartā hai guzar jā.egā  Ahmed Faraz\n",
      "1        itnā mānūs na ho ḳhalvat-e-ġham se apnī\\ntū kabhī ḳhud ko bhī dekhegā to Dar jā.egā  Ahmed Faraz\n",
      "2              Dūbte Dūbte kashtī ko uchhālā de duuñ\\nmaiñ nahīñ koī to sāhil pe utar jā.egā  Ahmed Faraz\n",
      "3          zindagī terī atā hai to ye jaane vaalā\\nterī baḳhshish tirī dahlīz pe dhar jā.egā  Ahmed Faraz\n",
      "4  zabt lāzim hai magar dukh hai qayāmat kā 'farāz'\\nzālim ab ke bhī na ro.egā to mar jā.egā  Ahmed Faraz\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the folder path\n",
    "folder_path = './dataset/ahmad-faraz' \n",
    "# 1. Define the folder path ('.' means the current fol \n",
    "all_dataframes = []\n",
    "\n",
    "# 2. Loop through every item in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "    # Check if it's a file and not a hidden system file (like .ipynb_checkpoints)\n",
    "    if os.path.isfile(file_path) and not filename.startswith('.'):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                # Read lines and strip whitespace\n",
    "                lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "            # 3. Pair two lines with a newline \\n\n",
    "            paired_data = []\n",
    "            for i in range(0, len(lines) - 1, 2):\n",
    "                combined = f\"{lines[i]}\\n{lines[i+1]}\"\n",
    "                paired_data.append(combined)\n",
    "\n",
    "            # 4. Create temporary dataframe for this file\n",
    "            temp_df = pd.DataFrame(paired_data, columns=['Aashar'])\n",
    "            temp_df['Shayer'] = 'Ahmed Faraz'\n",
    "            \n",
    "            all_dataframes.append(temp_df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # This skips files that aren't text-based (like images or notebooks)\n",
    "            print(f\"Skipping file {filename}: {e}\")\n",
    "\n",
    "# 5. Combine everything\n",
    "if all_dataframes:\n",
    "    df_final = pd.concat(all_dataframes, ignore_index=True)\n",
    "    print(f\"Successfully combined {len(all_dataframes)} files.\")\n",
    "    \n",
    "    # To see the first few rows with actual line breaks:\n",
    "    print(\"\\n--- Sample Output ---\")\n",
    "    print(df_final.head().to_string())\n",
    "else:\n",
    "    print(\"No files found to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca1f81-8295-4375-af91-6917057189c5",
   "metadata": {},
   "source": [
    "## a sample of row in data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6f588111-fe54-4b6b-b48c-6d71e844d3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ye kaun log haiñ maujūd terī mahfil meñ\n",
      "jo lālachoñ se tujhe mujh ko jal ke dekhte haiñ\n"
     ]
    }
   ],
   "source": [
    "print(df_final.loc[52,'Aashar'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38422c11-d843-4545-a863-3ac034a10947",
   "metadata": {},
   "source": [
    "## code to convert data frame into csv file and save it in local storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8e3c3f6f-b12f-4408-ab18-2b9fef49ad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('Faraz_gazal.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950478b1-8238-46ef-960c-d4d93fa27564",
   "metadata": {},
   "source": [
    "### reading the downloded dataframe, storing it in df1 and displaying a sampel of dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "209ee0c3-64ce-4848-96f5-1c2242642f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('collected_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "97988aae-e8ee-4e41-ad91-5b8a2c564f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aashar</th>\n",
       "      <th>Shayer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9304</th>\n",
       "      <td>kyā fā.eda hai dāva-e-ishq-e-husain se\\nsar me...</td>\n",
       "      <td>noon-meem-rashid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>qubūl kījiye lillāh tohfa-e-dil ko\\nnazar na k...</td>\n",
       "      <td>akbar-allahabadi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9647</th>\n",
       "      <td>ham ne dekhī hai vo ujlī saa.at\\nraat jab sher...</td>\n",
       "      <td>parveen-shakir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7396</th>\n",
       "      <td>jaate hue kahte ho qayāmat ko mileñge\\nkyā ḳhu...</td>\n",
       "      <td>mirza-ghalib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>guloñ meñ rañg bhare bād-e-nau-bahār chale\\nch...</td>\n",
       "      <td>faiz-ahmad-faiz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>ham pe nāzil huā sahīfa-e-ishq\\nsāhibān-e-kitā...</td>\n",
       "      <td>jigar-moradabadi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6229</th>\n",
       "      <td>is māh-e-chārdah kā chhape ishq kyūñke aah\\nab...</td>\n",
       "      <td>meer-taqi-meer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6243</th>\n",
       "      <td>muddat rahegī yaad tire chehre kī jhalak\\njalv...</td>\n",
       "      <td>meer-taqi-meer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7283</th>\n",
       "      <td>huuñ kashmakash-e-naz.a meñ haañ jazb-e-mohabb...</td>\n",
       "      <td>mirza-ghalib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>hai pahuñchnā apnā choTī tak muhāl\\nai talab n...</td>\n",
       "      <td>altaf-hussain-hali</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Aashar              Shayer\n",
       "9304  kyā fā.eda hai dāva-e-ishq-e-husain se\\nsar me...    noon-meem-rashid\n",
       "739   qubūl kījiye lillāh tohfa-e-dil ko\\nnazar na k...    akbar-allahabadi\n",
       "9647  ham ne dekhī hai vo ujlī saa.at\\nraat jab sher...      parveen-shakir\n",
       "7396  jaate hue kahte ho qayāmat ko mileñge\\nkyā ḳhu...        mirza-ghalib\n",
       "2698  guloñ meñ rañg bhare bād-e-nau-bahār chale\\nch...     faiz-ahmad-faiz\n",
       "5450  ham pe nāzil huā sahīfa-e-ishq\\nsāhibān-e-kitā...    jigar-moradabadi\n",
       "6229  is māh-e-chārdah kā chhape ishq kyūñke aah\\nab...      meer-taqi-meer\n",
       "6243  muddat rahegī yaad tire chehre kī jhalak\\njalv...      meer-taqi-meer\n",
       "7283  huuñ kashmakash-e-naz.a meñ haañ jazb-e-mohabb...        mirza-ghalib\n",
       "1211  hai pahuñchnā apnā choTī tak muhāl\\nai talab n...  altaf-hussain-hali"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9ccfc391-759b-4856-bda5-4876df705686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kyā fā.eda hai dāva-e-ishq-e-husain se\n",
      "sar meñ agar vo shauq-e-shahādat nahīñ rahā\n"
     ]
    }
   ],
   "source": [
    "print(df_final.loc[9304,'Aashar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b1b08d24-6d93-48ed-8eea-5ca1c362b025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ārzūoñ ke bahut ḳhvāb to dekho ho 'vasīm'\n",
      "jaane kis haal meñ be-dard zamāna rakkhe\n"
     ]
    }
   ],
   "source": [
    "print(df_final.loc[10500,'Aashar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eba828e-dfbe-4e27-b41d-d97ad280d243",
   "metadata": {},
   "outputs": [],
   "source": [
    "An End-to-End NLP Pipeline for Author Identification & Semantic Recommendation\n",
    "\n",
    "This project tackles the unique challenges of Roman-Urdu and provides two core functionalities:\n",
    "\n",
    "Multiclass Classification: Predicting the poet (Author ID)\n",
    "Semantic Recommendation: A vector-based retrieval system that recommends similar ash'aar (verses) \n",
    "Key Technical Highlights:\n",
    "Custom Preprocessing: Developed a specialized pipeline to handle phonetic inconsistencies and stop-word removal in Roman-Urdu.\n",
    "\n",
    "Feature Engineering: Utilized TF-IDF with N-gram character-level analysis to capture stylistic nuances of different poets.\n",
    "\n",
    "Vector Search: Implemented a Nearest-Neighbor retrieval system using Word2Vec/Transformer embeddings for poem similarity.\n",
    "\n",
    "Explainability: Integrated LIME/SHAP to visualize which words most influenced the model’s author prediction.\n",
    "\n",
    "Deployment: A fully interactive UI built with Streamlit for real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa8a8c-b078-40ff-a594-fa1205e92bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "An End-to-End NLP Pipeline for Author Identification & Semantic Recommendation This project tackles the unique challenges of Roman-Urdu and provides two core functionalities:Multiclass Classification: Predicting the poet(Author) Semantic Recommendation: A vector-based retrieval system that recommends similar ash'aar(verses) when given a single word\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
